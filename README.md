## Задание 1. Найдите внутри C# скрипта “коэффициент корреляции” и сделать выводы о том, как он влияет на обучение модели

Цель:
Исследовать влияние параметра forceMultiplier на процесс обучения модели.

Ход работы:
    - В скрипте обнаружен параметр forceMultiplier — корректирующий коэффициент силы.
    - Назначение:
        - Нормализует значения силы, предотвращая слишком малые величины (например, в диапазоне [-1, 1]).
        - Улучшает стабильность обучения, избегая "перегрузки" модели экстремальными значениями.

Вывод:
Коэффициент forceMultiplier напрямую влияет на сходимость модели, обеспечивая плавное обучение за счёт сбалансированных входных данных.

## Задание 2. Изменить параметры файла yaml-агента и определить какие параметры и как влияют на обучение модели. Привести описание не менее трех параметров.

Цель:
Изучить ключевые параметры файла конфигурации и их влияние на обучение.

Ход работы:
Изменены следующие параметры:
    - max_steps
        - Влияние: Определяет продолжительность обучения. Большие значения увеличивают время, но повышают точность модели.
        - Пример: max_steps: 1e6 позволяет агенту исследовать больше состояний среды.
    - trainer_type
        - Влияние: Выбор алгоритма обучения (например, PPO, SAC). Разные алгоритмы подходят для разных задач (дискретные/непрерывные действия).
        - Пример: trainer_type: ppо для задач с поэтапным вознаграждением.
    - num_layers
        - Влияние: Глубина нейронной сети. Увеличение слоёв улучшает способность модели к абстракции, но требует больше ресурсов.
        - Пример: num_layers: 3 для сложных сред с множеством переменных.

Вывод:
Оптимальная настройка этих параметров ускоряет обучение и повышает эффективность агента.

## Задание 3. Приведите примеры, для каких игровых задачи и ситуаций могут использоваться примеры 1 и 2 с ML-Agent’ом. В каких случаях проще использовать ML-агент, а не писать программную реализацию решения?

Цель:
Определить, в каких игровых сценариях ML-Agent предпочтительнее ручной реализации.

Примеры использования:
    - Модель 1 (преследование цели):
        - Задачи:
            - Преследование игрока вражескими NPC.
            -  Наведение снарядов (пули, огненные шары) на движущуюся цель.
        - Почему ML? Траектории движения сложно описать правилами из-за случайности и адаптивности цели.
    - Модель 2 (курьер между точками):
      - Задачи:
            - Доставка предметов с избеганием препятствий.
            - Патрулирование по динамическому маршруту.
        - Почему ML? Требуется адаптация к изменяющейся среде (например, новым препятствиям).
- Когда выбирать ML-Agent?
    - Среда содержит неопределённость (например, случайные препятствия).
    - Поведение должно обучаться на основе опыта (адаптация к игроку).
    - Жёсткие правила неэффективны (сложный ИИ противников).
- Преимущества:
    - Гибкость в изменяющихся условиях.
    - Автоматическая оптимизация стратегий.
    - Экономия времени на ручную настройку логики.
  
Итог:
Задания демонстрируют полный цикл работы с ML-Agent: от анализа параметров до применения в реальных игровых сценариях. Ключевой вывод — использование машинного обучения оправдано в задачах с высокой сложностью и нелинейностью.
