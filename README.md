Отчет по лабораторной работе #5 выполнил:
- Лыжин Лев Дмитриевич
- РИ-230913

| Задание | Выполнение | Баллы |
| ------ | ------ | ------ |
| Задание 1 | * | * |
| Задание 2 | * | * |
| Задание 3 | * | * |

Работу проверили:
- к.т.н., доцент Денисов Д.В.
- к.э.н., доцент Панов М.А.
- ст. преп., Фадеев В.О.

[![N|Solid](https://cldup.com/dTxpPi9lDf.thumb.png)](https://nodesource.com/products/nsolid)

[![Build Status](https://travis-ci.org/joemccann/dillinger.svg?branch=master)](https://travis-ci.org/joemccann/dillinger)

## Цель работы
Познакомиться с одной из первых моделей нейросетей - перцептроном и реализовать эту модель в Unity

## 1 Поиск агентом объекта на сцене
После сделанных пунктов по методичке объект на сцене Unity начал двигаться
![image](https://github.com/user-attachments/assets/37174e8d-601e-4ddc-89e0-74012fbb4c25)
1 модель

![image](https://github.com/user-attachments/assets/3ee3ae78-f210-447b-b485-4a69e1ab3d65)
3 модели

![image](https://github.com/user-attachments/assets/48abe578-f62c-439e-8fcf-efebedeac6b1)
9 моделей

![image](https://github.com/user-attachments/assets/ba4b2c2b-8a6b-467c-9bbe-0466a7dc4813)
27 моделей

## 2 Симулятор добычи ресурсов
После сделанных пунктов по методичке объект "перетаскивал" ресурсы

![image](https://github.com/user-attachments/assets/03eef6b1-147a-4e5c-8eaf-5545a946c1c4)
При запуске сцены, все объекты перетаскивали ресурсы абсолютно точно, скорее всего из-за того что я скачал проект с обученной моделью

![image](https://github.com/user-attachments/assets/e9355558-b6a8-474f-8d55-bf70560c8a54)
Графики оценки результатов обучения

## Задание 1. Найдите внутри C# скрипта “коэффициент корреляции” и сделать выводы о том, как он влияет на обучение модели

Цель:
Исследовать влияние параметра forceMultiplier на процесс обучения модели.

Ход работы:
    - В скрипте обнаружен параметр forceMultiplier — корректирующий коэффициент силы.
    - Назначение:
        - Нормализует значения силы, предотвращая слишком малые величины (например, в диапазоне [-1, 1]).
        - Улучшает стабильность обучения, избегая "перегрузки" модели экстремальными значениями.

Вывод:
Коэффициент forceMultiplier напрямую влияет на сходимость модели, обеспечивая плавное обучение за счёт сбалансированных входных данных.

## Задание 2. Изменить параметры файла yaml-агента и определить какие параметры и как влияют на обучение модели. Привести описание не менее трех параметров.

Цель:
Изучить ключевые параметры файла конфигурации и их влияние на обучение.

Ход работы:
Изменены следующие параметры:
    - max_steps
        - Влияние: Определяет продолжительность обучения. Большие значения увеличивают время, но повышают точность модели.
        - Пример: max_steps: 1e6 позволяет агенту исследовать больше состояний среды.
    - trainer_type
        - Влияние: Выбор алгоритма обучения (например, PPO, SAC). Разные алгоритмы подходят для разных задач (дискретные/непрерывные действия).
        - Пример: trainer_type: ppо для задач с поэтапным вознаграждением.
    - num_layers
        - Влияние: Глубина нейронной сети. Увеличение слоёв улучшает способность модели к абстракции, но требует больше ресурсов.
        - Пример: num_layers: 3 для сложных сред с множеством переменных.

Вывод:
Оптимальная настройка этих параметров ускоряет обучение и повышает эффективность агента.

## Задание 3. Приведите примеры, для каких игровых задачи и ситуаций могут использоваться примеры 1 и 2 с ML-Agent’ом. В каких случаях проще использовать ML-агент, а не писать программную реализацию решения?

Цель:
Определить, в каких игровых сценариях ML-Agent предпочтительнее ручной реализации.

Примеры использования:
    - Модель 1 (преследование цели):
        - Задачи:
            - Преследование игрока вражескими NPC.
            -  Наведение снарядов (пули, огненные шары) на движущуюся цель.
        - Почему ML? Траектории движения сложно описать правилами из-за случайности и адаптивности цели.
    - Модель 2 (курьер между точками):
      - Задачи:
            - Доставка предметов с избеганием препятствий.
            - Патрулирование по динамическому маршруту.
        - Почему ML? Требуется адаптация к изменяющейся среде (например, новым препятствиям).
- Когда выбирать ML-Agent?
    - Среда содержит неопределённость (например, случайные препятствия).
    - Поведение должно обучаться на основе опыта (адаптация к игроку).
    - Жёсткие правила неэффективны (сложный ИИ противников).
- Преимущества:
    - Гибкость в изменяющихся условиях.
    - Автоматическая оптимизация стратегий.
    - Экономия времени на ручную настройку логики.
  
Итог:
Задания демонстрируют полный цикл работы с ML-Agent: от анализа параметров до применения в реальных игровых сценариях. Ключевой вывод — использование машинного обучения оправдано в задачах с высокой сложностью и нелинейностью.
