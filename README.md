Отчет по лабораторной работе #5 выполнил:
- Лыжин Лев Дмитриевич
- РИ-230913

| Задание | Выполнение | Баллы |
| ------ | ------ | ------ |
| Задание 1 | * | * |
| Задание 2 | * | * |
| Задание 3 | * | * |

Работу проверили:
- к.т.н., доцент Денисов Д.В.
- к.э.н., доцент Панов М.А.
- ст. преп., Фадеев В.О.

[![N|Solid](https://cldup.com/dTxpPi9lDf.thumb.png)](https://nodesource.com/products/nsolid)

[![Build Status](https://travis-ci.org/joemccann/dillinger.svg?branch=master)](https://travis-ci.org/joemccann/dillinger)

## Цель работы
Познакомиться с одной из первых моделей нейросетей - перцептроном и реализовать эту модель в Unity

## 1 Поиск агентом объекта на сцене
Ход работы:
# Лабораторная работа по ML-Agents в Unity

## 1. Поиск агентом объекта на сцене

### Ход работы:

#### 1.1 Создание нового проекта
- Открыл Unity Hub и нажал "New Project"
- Выбрал шаблон **3D (Core)**
- Указал название проекта (`ML-Agents-Lab`) и создал его

#### 1.2 Добавление ML-Agents в проект
1. Скачал папку `ml-agents-release_19` из облачного хранилища
2. В Unity:
   - Перешел в `Window → Package Manager`
   - Нажал `+ → Add package from disk...`
   - Добавил файлы:
     - `com.unity.ml-agents/package.json`
     - `com.unity.ml-agents.extensions/package.json`
3. Проверил установку:
   - В компонентах появилась строка **ML Agents**

#### 1.3 Создание и настройка сцены
Объекты на сцене:
- Plane:    Position (0, 0, 0)     - основа сцены
- Cube:     Position (0, 0.5, 5)   - цель
- Sphere:   Position (0, 0.5, 0)   - агент
- 
Настройка агента (сферы)
- Я создал новый C# скрипт RollerAgent.cs и добавил в него код из методички. Прикрепил этот скрипт к сфере. Добавил к сфере необходимые компоненты:
    - Rigidbody (чтобы агент подчинялся физике).
    - Decision Requester (частота принятия решений – 5 раз в секунду).
    - Behavior Parameters (настроил параметры поведения, как в методичке).
Добавление конфигурации нейронной сети
    - Я скачал файл конфигурации (*.yaml) из материалов лабораторной работы.
    - Поместил его в корень проекта Unity (в папку Assets).

Запуск обучения ML-Agent
- Я открыл Anaconda Prompt и активировал окружение ml-agents.
- Перешел в папку проекта Unity и выполнил команду:bash; Copy mlagents-learn config.yaml --run-id=RollerBall_Test
- Unity автоматически перешел в режим обучения, и в консоли начали появляться данные о процессе.

Тестирование работы агента
- В Unity я запустил сцену и наблюдал за поведением сферы.
- Сначала агент двигался хаотично, но постепенно начал обучаться и двигаться к кубу.

Масштабирование симуляции (3, 9, 27 агентов)
- Я создал несколько копий модели "Плоскость-Сфера-Куб":
    - 3 модели – агенты обучались быстрее, так как было меньше конкуренции.
    - 9 моделей – обучение замедлилось, но агенты все равно находили цель.
    - 27 моделей – симуляция стала сложнее, но некоторые агенты все равно достигали куба.

Результаты обучения отображались в консоли ML-Agents в виде графиков:
- Reward (награда) постепенно росла.
- Episode Length (длительность эпизода) уменьшалась, что означало улучшение стратегии агентов.

После сделанных пунктов по методичке объект на сцене Unity начал двигаться
![image](https://github.com/user-attachments/assets/37174e8d-601e-4ddc-89e0-74012fbb4c25)
1 модель

![image](https://github.com/user-attachments/assets/3ee3ae78-f210-447b-b485-4a69e1ab3d65)
3 модели

![image](https://github.com/user-attachments/assets/48abe578-f62c-439e-8fcf-efebedeac6b1)
9 моделей

![image](https://github.com/user-attachments/assets/ba4b2c2b-8a6b-467c-9bbe-0466a7dc4813)
27 моделей

## 2 Симулятор добычи ресурсов
После сделанных пунктов по методичке объект "перетаскивал" ресурсы

![image](https://github.com/user-attachments/assets/03eef6b1-147a-4e5c-8eaf-5545a946c1c4)
При запуске сцены, все объекты перетаскивали ресурсы абсолютно точно, скорее всего из-за того что я скачал проект с обученной моделью

![image](https://github.com/user-attachments/assets/e9355558-b6a8-474f-8d55-bf70560c8a54)
Графики оценки результатов обучения

## Задание 1. Найдите внутри C# скрипта “коэффициент корреляции” и сделать выводы о том, как он влияет на обучение модели

Цель:
Исследовать влияние параметра forceMultiplier на процесс обучения модели.

Ход работы:
    - В скрипте обнаружен параметр forceMultiplier — корректирующий коэффициент силы.
    - Назначение:
        - Нормализует значения силы, предотвращая слишком малые величины (например, в диапазоне [-1, 1]).
        - Улучшает стабильность обучения, избегая "перегрузки" модели экстремальными значениями.

Вывод:
Коэффициент forceMultiplier напрямую влияет на сходимость модели, обеспечивая плавное обучение за счёт сбалансированных входных данных.

## Задание 2. Изменить параметры файла yaml-агента и определить какие параметры и как влияют на обучение модели. Привести описание не менее трех параметров.

Цель:
Изучить ключевые параметры файла конфигурации и их влияние на обучение.

Ход работы:
Изменены следующие параметры:
    - max_steps
        - Влияние: Определяет продолжительность обучения. Большие значения увеличивают время, но повышают точность модели.
        - Пример: max_steps: 1e6 позволяет агенту исследовать больше состояний среды.
    - trainer_type
        - Влияние: Выбор алгоритма обучения (например, PPO, SAC). Разные алгоритмы подходят для разных задач (дискретные/непрерывные действия).
        - Пример: trainer_type: ppо для задач с поэтапным вознаграждением.
    - num_layers
        - Влияние: Глубина нейронной сети. Увеличение слоёв улучшает способность модели к абстракции, но требует больше ресурсов.
        - Пример: num_layers: 3 для сложных сред с множеством переменных.

Вывод:
Оптимальная настройка этих параметров ускоряет обучение и повышает эффективность агента.

## Задание 3. Приведите примеры, для каких игровых задачи и ситуаций могут использоваться примеры 1 и 2 с ML-Agent’ом. В каких случаях проще использовать ML-агент, а не писать программную реализацию решения?

Цель:
Определить, в каких игровых сценариях ML-Agent предпочтительнее ручной реализации.

Примеры использования:
    - Модель 1 (преследование цели):
        - Задачи:
            - Преследование игрока вражескими NPC.
            -  Наведение снарядов (пули, огненные шары) на движущуюся цель.
        - Почему ML? Траектории движения сложно описать правилами из-за случайности и адаптивности цели.
    - Модель 2 (курьер между точками):
      - Задачи:
            - Доставка предметов с избеганием препятствий.
            - Патрулирование по динамическому маршруту.
        - Почему ML? Требуется адаптация к изменяющейся среде (например, новым препятствиям).
- Когда выбирать ML-Agent?
    - Среда содержит неопределённость (например, случайные препятствия).
    - Поведение должно обучаться на основе опыта (адаптация к игроку).
    - Жёсткие правила неэффективны (сложный ИИ противников).
- Преимущества:
    - Гибкость в изменяющихся условиях.
    - Автоматическая оптимизация стратегий.
    - Экономия времени на ручную настройку логики.
  
Итог:
Задания демонстрируют полный цикл работы с ML-Agent: от анализа параметров до применения в реальных игровых сценариях. Ключевой вывод — использование машинного обучения оправдано в задачах с высокой сложностью и нелинейностью.
